{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Electronics Recommender System</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Recommender Systems: Addressing Information Overload\n",
    "We live in an era saturated with content, where the sheer volume of movies, news articles, shopping products, and websites overwhelms individual attention spans. The average Google search yields over a million results, yet how often do we venture beyond the first page of links? This phenomenon, known as the \"long tail problem,\" highlights how a small fraction of content receives disproportionate attention, while the majority remains undiscovered.\n",
    "\n",
    "In the face of this challenge, service providers must ask: \"How do I curate a manageable selection of content for users that is both relevant and desired?\" Thankfully, decades of research have produced a solution: recommender systems.\n",
    "\n",
    "Understanding Recommender Systems\n",
    "Recommender systems predict a user's preference for an item, allowing service providers to offer a tailored selection of content, thereby enhancing user engagement and broadening content exploration.\n",
    "\n",
    "Fundamental Concepts\n",
    "Terminology: Users, Items, and Ratings\n",
    "In the realm of recommender systems, two primary entities exist: Users and Items.\n",
    "\n",
    "Items are the content being consumedâ€”movies, articles, products, etc. They remain passive, with fixed properties.\n",
    "Users interact with these items, providing ratings based on their preferences. Ratings can be explicit (e.g., giving a movie a star rating) or implicit (e.g., watching a movie without rating it directly).\n",
    "Implementing Content-Based Filtering: An Example\n",
    "Let's delve into one of the primary methods employed in recommender systems: content-based filtering. In this context, we'll focus on building an \"Electronics Recommender System.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Measuring Similarity \n",
    "\n",
    "<br></br>\n",
    "\n",
    "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Cosine_similarity.jpg\"\n",
    "     alt=\"Cosine Similarity \"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=600px/>\n",
    "Measuring the similarity between the ratings of two users (A) and (B) for the books 'Harry Potter and the Philosopher's Stone' and 'The Diary of a Young Girl', using the Cosine similarity metric.  \n",
    "</div>\n",
    "\n",
    "\n",
    "Having learnt about the entities which exist within recommender systems, we may wonder how they function. While this is something that we'll learn throughout this entire train, one fundamental principal that we need to understand is that recommender systems are built up by utilising the _relations_ which  exist between items and users. As such, these systems always need a mechanism to measure how related or _similar_ a user is to another user, or an item is to another item. \n",
    "\n",
    "We accomplish this measurement of similarity through, you guessed it, a _similarity metric_.  \n",
    "\n",
    "Generally speaking, a similarity metric can be thought of as being the inverse of a distance measure: if two things are considered to be very similar they should be assigned a high similarity value (close to 1), while dissimilar items should receive a low similarity value (close to zero). Other [important properties](https://online.stat.psu.edu/stat508/lesson/1b/1b.2/1b.2.1) include:\n",
    " - (Symmetry) $Sim(A,B) = Sim(B,A)$ \n",
    " - (Identity) $Sim(A,A) = 1$\n",
    " - (Uniqueness) $Sim(A,B) = 1 \\leftrightarrow A = B$\n",
    " \n",
    "While there are many similarity metrics to choose from when building a recommender system (and more than one can certainly be used simultaneously), a popular choice is the **Cosine similarity**. We won't go into the fundamental trig here (we hope that you remember this from high school), but recall that as an angle becomes smaller (approaching $0^o$) the value of its cosine increases. Conversely, as the angle increases the cosine value decreases. It turns out that this behavior makes the cosine of the angle between two p-dimensional vectors desirable as a [similarity metric](https://en.wikipedia.org/wiki/Cosine_similarity) which can easily be computed.\n",
    "\n",
    "Using the figure above to help guide our understanding, the Cosine similarity between two p-dimensional vectors ${A}$ and $B$ can be given as:\n",
    "\n",
    "$$ \\begin{align}\n",
    "Sim(A,B)  &= \\frac{A \\cdot B}{||A|| \\times ||B||} \\\\ \\\\\n",
    "& = \\frac{\\sum_{i=1}^{p}A_{i}B_{i}}{\\sqrt{{\\sum_{i=1}^{p}A_{i}^2}} \\sqrt{\\sum_{i=1}^{p}B_{i}^2}}, \\\\\n",
    "\\end{align} $$ \n",
    "  \n",
    "\n",
    "To make things a little more concrete, let's work out the cosine similarity using our provided example above. Here, each vector represents the ratings given by one of two *users*, $A$ and $B$, who have each rated two books (rating#1 $ \\rightarrow r_1$, and rating#2 $ \\rightarrow r_2$). To work out how similar these two users are based on their supplied ratings, we can use the Cosine similarity definition as follows:   \n",
    "\n",
    "\n",
    "$$ \\begin{align}\n",
    "Sim(A,B)  & = \\frac{(A_{r1} \\times B_{r1})+(A_{r2} \\times B_{r2})}{\\sqrt{A_{r1}^2 + A_{r2}^2} \\times \\sqrt{B_{r1}^2 + B_{r2}^2}} \\\\ \\\\\n",
    "& = \\frac{(3 \\times 5) + (4 \\times 2)}{\\sqrt{9 + 16} \\times \\sqrt{25 + 4}} \\\\ \\\\\n",
    "& = \\frac{23}{26.93} \\\\ \\\\\n",
    "& = 0.854\n",
    "\\end{align} $$\n",
    "\n",
    "It would be a pain to work this out manually each time! Thankfully, we can obtain this same result using the `cosine_similarity` function provided to us in `sklearn`. \n",
    "\n",
    "As usual before we can go ahead and use this function we need to import the libraries that we will need.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/emmau/Downloads/Electronics-Product-Recommendation/Venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "##Importing Libraries\n",
    "\n",
    "\n",
    "# Import our regular old heroes \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp # <-- The sister of Numpy, used in our code for numerical efficientcy. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Entity featurization and similarity computation\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from surprise import SVD, Reader, Dataset\n",
    "\n",
    "# Libraries used during sorting procedures.\n",
    "import operator # <-- Convienient item retrieval during iteration \n",
    "import heapq # <-- Efficient sorting of large lists\n",
    "\n",
    "# Imported for our sanity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
